{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Part2"
      ],
      "metadata": {
        "id": "MxEiW2A-6Y5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# Constants\n",
        "UNK = \"UNK\"\n",
        "start = \"<s>\"\n",
        "end = \"</s>\"\n",
        "\n",
        "#----------------------------------------\n",
        "#  Data input\n",
        "#----------------------------------------\n",
        "\n",
        "def readFileToCorpus(f):\n",
        "    if os.path.isfile(f):\n",
        "        corpus = []\n",
        "        with open(f, \"r\") as file:\n",
        "            for line in file:\n",
        "                corpus.append(line.split())\n",
        "        return corpus\n",
        "    else:\n",
        "        print(f\"Error: {f} does not exist\")\n",
        "        sys.exit()\n",
        "\n",
        "def preprocess(corpus):\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "        for word in sen:\n",
        "            freqDict[word] += 1\n",
        "    for sen in corpus:\n",
        "        for i in range(len(sen)):\n",
        "            if freqDict[sen[i]] < 2:\n",
        "                sen[i] = UNK\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    return corpus\n",
        "\n",
        "def preprocessTest(vocab, corpus):\n",
        "    for sen in corpus:\n",
        "        for i in range(len(sen)):\n",
        "            if sen[i] not in vocab:\n",
        "                sen[i] = UNK\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    return corpus\n",
        "\n",
        "#----------------------------------------------\n",
        "# Language Models\n",
        "#----------------------------------------------\n",
        "\n",
        "class LanguageModel:\n",
        "    def __init__(self, corpus):\n",
        "        pass\n",
        "\n",
        "    def generateSentence(self):\n",
        "        pass\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        pass\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        log_sum = 0.0\n",
        "        total_words = 0\n",
        "        for sen in corpus:\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "            if prob == 0:\n",
        "                return float('inf')\n",
        "            log_sum += math.log(prob, 2)\n",
        "            total_words += len(sen) - 1  # exclude <s>\n",
        "        if total_words == 0:\n",
        "            return float('inf')\n",
        "        return 2 ** (-log_sum / total_words)\n",
        "\n",
        "    def generateSentencesToFile(self, n, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            for _ in range(n):\n",
        "                sen = self.generateSentence()\n",
        "                prob = self.getSentenceProbability(sen)\n",
        "                f.write(f\"{prob} {' '.join(sen)}\\n\")\n",
        "\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        self.counts=defaultdict(float)\n",
        "        self.total=0.0\n",
        "        for sen in corpus:\n",
        "            for w in sen:#in case of unigrams start not counted\n",
        "                if w==start:\n",
        "                    continue\n",
        "                self.counts[w]+= 1\n",
        "                self.total+= 1\n",
        "        self.vocab=list(self.counts.keys())\n",
        "\n",
        "    def prob(self, word):\n",
        "        return self.counts[word] / self.total if self.total > 0 else 0# count of words/total wors in corpous\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence=[start]#initialized with <s>\n",
        "        while True:\n",
        "            rand=random.random()#between 0 and 1,used to select a word based on its prob\n",
        "            for w in self.vocab:\n",
        "                rand-=self.prob(w)#sub the prob of that word from rand\n",
        "                if rand <= 0:\n",
        "                    sentence.append(w)#that word is selected and appended to the sen\n",
        "                    if w==end:#if <\\s>,returns the completed sentence\n",
        "                        return sentence\n",
        "                    break\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob=1.0\n",
        "        for w in sen[1:]:\n",
        "            if self.counts[w]==0:\n",
        "                return 0.0\n",
        "            prob*=self.prob(w)\n",
        "        return prob\n",
        "\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        self.counts=defaultdict(float)\n",
        "        self.total=0.0\n",
        "        for sen in corpus:\n",
        "            for w in sen:\n",
        "                if w==start:\n",
        "                    continue\n",
        "                self.counts[w]+=1\n",
        "                self.total+=1\n",
        "        self.V=len(self.counts)\n",
        "\n",
        "    def prob(self, word):#add 1 smoothing\n",
        "        res= (self.counts.get(word, 0) + 1)/ (self.total + self.V)\n",
        "        return res\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sen=[start]\n",
        "        while True:\n",
        "            words=list(self.counts.keys()) + [end]\n",
        "            rand=random.random()\n",
        "            for w in words:\n",
        "                p=self.prob(w)\n",
        "                rand-=p\n",
        "                if rand <=0:\n",
        "                    sen.append(w)\n",
        "                    if w==end:\n",
        "                        return sen\n",
        "                    break\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob=1.0\n",
        "        for w in sen[1:]:\n",
        "            prob*=self.prob(w)\n",
        "        return prob\n",
        "\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        self.counts=defaultdict(float)\n",
        "        self.total=0.0\n",
        "        for sen in corpus:\n",
        "            for word in sen:\n",
        "                if word==start:\n",
        "                    continue\n",
        "                self.counts[word]+=1\n",
        "                self.total+=1\n",
        "        self.V=len(self.counts)\n",
        "\n",
        "    def prob(self, word):#add 1 smoothing\n",
        "        return (self.counts.get(word, 0) + 1) / (self.total + self.V)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence=[start]\n",
        "        while True:\n",
        "            words=list(self.counts.keys()) + [end]\n",
        "            rand=random.random()\n",
        "            for word in words:\n",
        "                p=self.prob(word)\n",
        "                rand-=p\n",
        "                if rand <=0:\n",
        "                    sentence.append(word)\n",
        "                    if word==end:\n",
        "                        return sentence\n",
        "                    break\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob=1.0\n",
        "        for word in sen[1:]:\n",
        "            prob*=self.prob(word)\n",
        "        return prob\n",
        "\n",
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        self.bigram_counts=defaultdict(float)\n",
        "        self.context_counts=defaultdict(float)\n",
        "        for sen in corpus:\n",
        "            for i in range(1, len(sen)):\n",
        "                prev, curr=sen[i-1],sen[i]\n",
        "                self.bigram_counts[(prev, curr)]+=1\n",
        "                self.context_counts[prev]+=1\n",
        "\n",
        "    def prob(self, prev, curr):\n",
        "        if self.context_counts[prev]==0:\n",
        "            return 0.0\n",
        "        return self.bigram_counts.get((prev, curr), 0.0) / self.context_counts[prev]\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence=[start]\n",
        "        current=start\n",
        "        while True:\n",
        "            #collects all words that have ever been seen following the current word curr in your training data.\n",
        "            next_words=[curr for (p, curr) in self.bigram_counts if p==current]\n",
        "            if not next_words:\n",
        "                sentence.append(end)\n",
        "                break\n",
        "            probs=[self.prob(current, w) for w in next_words]\n",
        "            total=sum(probs)#rep the total prob mass for all possible continuations from curr\n",
        "            if total==0:\n",
        "                sentence.append(end)\n",
        "                break\n",
        "            rand=random.random() * total\n",
        "            for i, w in enumerate(next_words):\n",
        "                rand-=probs[i]\n",
        "                if rand <=0:\n",
        "                    current=w\n",
        "                    sentence.append(w)\n",
        "                    break\n",
        "            if current==end:\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob=1.0\n",
        "        for i in range(1, len(sen)):\n",
        "            prev, curr=sen[i-1], sen[i]\n",
        "            p=self.prob(prev, curr)\n",
        "            if p==0:\n",
        "                return 0.0\n",
        "            prob*=p\n",
        "        return prob\n",
        "\n",
        "\n",
        "class SmoothedBigramModelLI(LanguageModel):#linear interpolation\n",
        "    def __init__(self, corpus):\n",
        "        self.bigram_counts=defaultdict(float)\n",
        "        self.context_counts=defaultdict(float)\n",
        "        self.unigram_counts=defaultdict(float)\n",
        "        self.total_uni=0.0\n",
        "        for sen in corpus:\n",
        "            for w in sen:#for all words\n",
        "                if w!=start:\n",
        "                    self.unigram_counts[w]+=1\n",
        "                    self.total_uni+=1\n",
        "            for i in range(1, len(sen)):\n",
        "                prev, curr = sen[i-1], sen[i]\n",
        "                self.bigram_counts[(prev, curr)]+=1\n",
        "                self.context_counts[prev] += 1\n",
        "        self.lam1=0.5#lambdas\n",
        "        self.lam2=0.5\n",
        "\n",
        "    def prob(self, prev, curr):\n",
        "        p_bi=self.bigram_counts.get((prev, curr),0.0) / self.context_counts[prev] if self.context_counts[prev]>0 else 0.0\n",
        "        p_uni=self.unigram_counts[curr] / self.total_uni if self.total_uni >0 else 0.0\n",
        "        res= self.lam1*p_bi + self.lam2*p_uni\n",
        "        return res\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sen=[start]\n",
        "        curr=start\n",
        "        while True:\n",
        "            possible_words=list(self.unigram_counts.keys()) + [end]\n",
        "            probs=[self.prob(curr, w) for w in possible_words]\n",
        "            total=sum(probs)\n",
        "            if total==0:\n",
        "                sen.append(end)\n",
        "                break\n",
        "            probs=[p / total for p in probs]\n",
        "            rand=random.random()\n",
        "            for i, w in enumerate(possible_words):\n",
        "                rand-=probs[i]\n",
        "                if rand <=0:\n",
        "                    curr=w\n",
        "                    sen.append(w)\n",
        "                    break\n",
        "            if curr==end:\n",
        "                break\n",
        "        return sen\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob=1.0\n",
        "        for i in range(1, len(sen)):\n",
        "            prev,curr=sen[i-1], sen[i]\n",
        "            prob*=self.prob(prev, curr)\n",
        "        return prob\n",
        "\n",
        "#-------------------------------------------\n",
        "# Main\n",
        "#-------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    trainCorpus = readFileToCorpus('train.txt')\n",
        "    trainCorpus = preprocess(trainCorpus)\n",
        "\n",
        "    vocab = set()\n",
        "    for sen in trainCorpus:\n",
        "        for word in sen:\n",
        "            vocab.add(word)\n",
        "\n",
        "    posTestCorpus = readFileToCorpus('pos_test.txt')\n",
        "    negTestCorpus = readFileToCorpus('neg_test.txt')\n",
        "    posTestCorpus = preprocessTest(vocab, posTestCorpus)\n",
        "    negTestCorpus = preprocessTest(vocab, negTestCorpus)\n",
        "\n",
        "    #models training\n",
        "    uni_model = UnigramModel(trainCorpus)\n",
        "    print(\"done\")\n",
        "    smooth_uni_model = SmoothedUnigramModel(trainCorpus)\n",
        "    print(\"done\")\n",
        "    bi_model = BigramModel(trainCorpus)\n",
        "    print(\"done\")\n",
        "    smooth_bi_model = SmoothedBigramModelLI(trainCorpus)\n",
        "    print(\"done\")\n",
        "\n",
        "    #sentences generation\n",
        "    uni_model.generateSentencesToFile(20, 'unigram_output.txt')\n",
        "    print(\"done\")\n",
        "    smooth_uni_model.generateSentencesToFile(20, 'smooth_unigram_output.txt')\n",
        "    print(\"done\")\n",
        "    bi_model.generateSentencesToFile(20, 'bigram_output.txt')\n",
        "    print(\"done\")\n",
        "    smooth_bi_model.generateSentencesToFile(20, 'smooth_bi_li_output.txt')\n",
        "    print(\"done\")\n",
        "\n",
        "\n",
        "    models = [uni_model, smooth_uni_model, bi_model, smooth_bi_model]\n",
        "    model_names = ['Unigram', 'SmoothedUnigram', 'Bigram', 'SmoothedBigramLI']\n",
        "    for model, name in zip(models, model_names):\n",
        "        pos_ppl = model.getCorpusPerplexity(posTestCorpus)\n",
        "        neg_ppl = model.getCorpusPerplexity(negTestCorpus)\n",
        "        print(f\"{name} pos perplexity: {pos_ppl}\")\n",
        "        print(f\"{name} neg perplexity: {neg_ppl}\")\n",
        "'''\n",
        "1-\n",
        "\n",
        "The unigram model’s sentence length is controlled solely by the chance of drawing </s>, while bigram models\n",
        "condition on the previous word, leading to more context-driven and realistic sentence boundaries\n",
        "2-\n",
        "Unsmoothed models assign zero probability to unseen n-grams, leading to drastic differences.\n",
        "Smoothed models spread probability mass to unseen events,leading to more reasonable sentence probabilities.thus\n",
        "resulting in less drastic differences,\n",
        "\n",
        "3-\n",
        "The smoothed bigram model produces more realistic sentences by handling unseen bigrams through\n",
        "interpolation, avoiding abrupt endings.\n",
        "4-\n",
        "Perplexities:\n",
        "\n",
        "Unigram: pos= 628.67,, neg=612.26\n",
        "SmoothedUnigram: pos=631.56, neg=615.36\n",
        "Bigram: pos=inf, neg=inf (due to OOV or zero prob bigrams)\n",
        "SmoothedBigramLI: pos=243.50, neg=251.22\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "UcuNfxh-JvFI",
        "outputId": "51e6975a-ce07-44dd-a7d5-36bf70dbed4f",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "Unigram pos perplexity: 628.6696007319457\n",
            "Unigram neg perplexity: 612.2628559648674\n",
            "SmoothedUnigram pos perplexity: 631.5562305194113\n",
            "SmoothedUnigram neg perplexity: 615.3626467739131\n",
            "Bigram pos perplexity: inf\n",
            "Bigram neg perplexity: inf\n",
            "SmoothedBigramLI pos perplexity: 243.5032538798665\n",
            "SmoothedBigramLI neg perplexity: 251.21906135625858\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1-\\n \\nThe unigram model’s sentence length is controlled solely by the chance of drawing </s>, while bigram models \\ncondition on the previous word, leading to more context-driven and realistic sentence boundaries\\n2-\\nUnsmoothed models assign zero probability to unseen n-grams, leading to drastic differences.\\nSmoothed models spread probability mass to unseen events,leading to more reasonable sentence probabilities.thus\\nresulting in less drastic differences,  \\n\\n3-\\nThe smoothed bigram model produces more realistic sentences by handling unseen bigrams through \\ninterpolation, avoiding abrupt endings.\\n4-\\nPerplexities:\\n\\nUnigram: pos= 628.67,, neg=612.26\\nSmoothedUnigram: pos=631.56, neg=615.36\\nBigram: pos=inf, neg=inf (due to OOV or zero prob bigrams)\\nSmoothedBigramLI: pos=243.50, neg=251.22\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}